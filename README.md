# arXiv curation - AI Safety

Curation of arXiv papers related to AI Safety, categorized and summarized by LLM.

## Overview (2026-01-07)
- **Agentic & Long-Horizon Risks**: This week's papers address agentic and long-horizon risks by introducing a novel framework, ASG-SI, for auditable and governable self-improving agentic LLMs. The key developments lie in its ability to compile improvements into verifiable skill graphs with reconstructible rewards, directly tackling issues like reward hacking and behavioral drift that are critical for controlling advanced AI systems.
- **Alignment**: This week's papers reveal new vulnerabilities and mitigation strategies for AI alignment. One study shows that emoji sequences can exploit safety mechanisms in LLMs, while another introduces GARDO to reinforce diffusion models and prevent reward hacking. Additionally, a new information-theoretic method, DIR, aims to eliminate inductive biases in reward models, thereby improving alignment with human values.
- **Evaluation & Benchmarking**: This week's papers in Evaluation & Benchmarking introduce a new benchmark, JMedEthicBench, specifically designed to test the medical safety of Japanese Large Language Models in multi-turn conversations. This development highlights the need for specialized evaluations that capture the unique safety challenges arising from complex, extended dialogue, rather than just single-turn queries.
- **Interpretability**: This week's papers highlight critical concerns regarding the reliability of current interpretability methods. Specifically, research on feature extraction and steering using sparse autoencoders in LLMs reveals significant fragility, indicating that these techniques might not be as generalizable or robust as previously assumed for AI safety applications.
- **Misuse & Security**: This week's papers highlight significant advancements and persistent challenges in LLM security. Researchers are developing novel defensive strategies, such as guardrail models trained on compressed conversations and multimodal frameworks to prevent prompt injection attacks. However, new attack vectors, like multilingual hidden prompt injection and equation-solving based jailbreaks, demonstrate the ongoing arms race and the need for continuous refinement of safety filters across diverse attack scenarios.
- **Robustness & Generalization**: This week's papers introduce novel methods for improving LLM robustness against adversarial attacks. One paper presents RAILS, a gradient-free and prior-free optimization framework that leverages an auto-regressive loss and history-based selection for effective jailbreaking, even against closed-source models. The other paper, JPU, focuses on a defense mechanism that rectifies dynamic jailbreak paths by mining on-policy adversarial samples, aiming to enhance LLM resistance without compromising utility.


## Latest Papers (2026-01-07)

### Agentic & Long-Horizon Risks
_This week's papers address agentic and long-horizon risks by introducing a novel framework, ASG-SI, for auditable and governable self-improving agentic LLMs. The key developments lie in its ability to compile improvements into verifiable skill graphs with reconstructible rewards, directly tackling issues like reward hacking and behavioral drift that are critical for controlling advanced AI systems._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2025-12-28 | [Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory](http://arxiv.org/abs/2512.23760v1) | LLM, Reinforcement Learning, Agents, Security | ASG-SI is a framework that makes self-improving agentic LLMs auditable and governable by compiling improvements into verifiable skill graphs with reconstructible rewards, directly addressing security and control challenges like reward hacking and behavioral drift. |

### Alignment
_This week's papers reveal new vulnerabilities and mitigation strategies for AI alignment. One study shows that emoji sequences can exploit safety mechanisms in LLMs, while another introduces GARDO to reinforce diffusion models and prevent reward hacking. Additionally, a new information-theoretic method, DIR, aims to eliminate inductive biases in reward models, thereby improving alignment with human values._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-07 | [What Matters For Safety Alignment?](http://arxiv.org/abs/2601.03868v1) | LLM | This paper empirically identifies intrinsic model characteristics that improve safety alignment in LLMs and LRMs, reveals vulnerabilities in text-completion interfaces, and suggests safety should be an explicit optimization objective during training. |
| 2026-01-06 | [Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models](http://arxiv.org/abs/2601.03388v1) | LLM | This paper demonstrates a causal relationship between metaphors in training data and the emergent cross-domain misalignment of large reasoning models, suggesting metaphors can induce generalization of learned misaligned patterns. |
| 2026-01-05 | [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](http://arxiv.org/abs/2601.02036v1) | LLM, Reinforcement Learning | GDRO is a novel post-training method for diffusion models that achieves efficient, offline group-level reward alignment by addressing sampling efficiency and stochasticity issues inherent in rectified flow models. |
| 2026-01-02 | [Emoji-Based Jailbreaking of Large Language Models](http://arxiv.org/abs/2601.00936v1) | LLM | This study empirically demonstrates that emoji sequences can be used as adversarial prompts to bypass safety alignment mechanisms in Large Language Models, highlighting vulnerabilities in prompt-level safety pipelines. |
| 2025-12-30 | [GARDO: Reinforcing Diffusion Models without Reward Hacking](http://arxiv.org/abs/2512.24138v1) | Reinforcement Learning | The paper introduces GARDO, a framework that mitigates reward hacking and enhances generation diversity in RL-tuned diffusion models by employing selective, adaptive regularization and diversity-aware reward amplification. |
| 2025-12-29 | [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](http://arxiv.org/abs/2512.23461v1) | LLM, Reinforcement Learning | This paper introduces DIR, an information-theoretic debiasing method for reward models that minimizes the influence of inductive biases like response length and sycophancy by optimizing mutual information, thereby enhancing RLHF performance and improving LLM alignment with human values. |

### Evaluation & Benchmarking
_This week's papers in Evaluation & Benchmarking introduce a new benchmark, JMedEthicBench, specifically designed to test the medical safety of Japanese Large Language Models in multi-turn conversations. This development highlights the need for specialized evaluations that capture the unique safety challenges arising from complex, extended dialogue, rather than just single-turn queries._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-07 | [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](http://arxiv.org/abs/2601.04389v1) | LLM, Security | MiJaBench is a new bilingual benchmark that reveals how LLM safety alignment is not generalized but hierarchical across demographic groups, with model scaling potentially exacerbating these biases. |
| 2026-01-05 | [Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics](http://arxiv.org/abs/2601.02200v1) | LLM, Agents | This paper introduces and validates a metric (CodeHealth) for quantifying AI-friendliness in code, demonstrating that human-friendly code is also more robust to AI-driven refactoring. |
| 2026-01-04 | [JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models](http://arxiv.org/abs/2601.01627v1) | LLM, Healthcare | JMedEthicBench is a novel multi-turn conversational benchmark for evaluating the medical safety of Japanese LLMs, revealing vulnerabilities in domain-specific models and highlighting the distinct safety challenges of multi-turn interactions. |

### Interpretability
_This week's papers highlight critical concerns regarding the reliability of current interpretability methods. Specifically, research on feature extraction and steering using sparse autoencoders in LLMs reveals significant fragility, indicating that these techniques might not be as generalizable or robust as previously assumed for AI safety applications._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-06 | [When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability](http://arxiv.org/abs/2601.03047v1) |  | This paper critically examines the reliability and generalizability of sparse autoencoders for mechanistic interpretability in LLMs, finding significant fragility in feature extraction and steering, which raises concerns for their application in AI safety. |

### Misuse & Security
_This week's papers highlight significant advancements and persistent challenges in LLM security. Researchers are developing novel defensive strategies, such as guardrail models trained on compressed conversations and multimodal frameworks to prevent prompt injection attacks. However, new attack vectors, like multilingual hidden prompt injection and equation-solving based jailbreaks, demonstrate the ongoing arms race and the need for continuous refinement of safety filters across diverse attack scenarios._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-07 | [HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense](http://arxiv.org/abs/2601.04034v1) | LLM, Agents, Security | HoneyTrap is a novel deceptive LLM defense framework that uses collaborative agents to detect and misdirect jailbreak attacks, increasing attacker resource consumption and reducing attack success rates. |
| 2026-01-07 | [ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification](http://arxiv.org/abs/2601.03600v1) | LLM, Security | This paper proposes ALERT, an efficient zero-shot jailbreak detection method that amplifies internal feature discrepancies to identify malicious prompts without relying on pre-existing attack templates. |
| 2026-01-06 | [Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning](http://arxiv.org/abs/2601.02983v1) | LLM, Reinforcement Learning | This paper proposes a novel Frequency Time-Group Relative Policy Optimization (FT-GRPO) method using audio large language models (ALLMs) and structured chain-of-thought rationales to achieve interpretable and robust detection of all types of audio deepfakes. |
| 2026-01-06 | [Adversarial Contrastive Learning for LLM Quantization Attacks](http://arxiv.org/abs/2601.02680v1) | LLM, Security | The paper introduces Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that enhances the effectiveness of malicious LLM behaviors by explicitly maximizing the probability gap between benign and harmful responses. |
| 2026-01-06 | [TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering](http://arxiv.org/abs/2601.03300v1) | LLM, Security | TRYLOCK introduces a defense-in-depth architecture combining weight-level alignment, activation-level steering, adaptive strength selection, and input canonicalization to significantly reduce LLM jailbreak success rates without substantial usability trade-offs. |
| 2026-01-06 | [Extracting books from production language models](http://arxiv.org/abs/2601.02671v1) | LLM | This paper demonstrates that even production LLMs with safety measures can leak copyrighted training data through extraction attacks, challenging the assumption that such safeguards prevent memorization exposure. |
| 2026-01-05 | [Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models](http://arxiv.org/abs/2601.02002v1) | LLM | This paper explores and evaluates automated prompting strategies for detecting memorization and data leakage in Large Language Models used for recommendations, outperforming manual prompting and unsupervised methods. |
| 2026-01-05 | [Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization](http://arxiv.org/abs/2601.01747v2) | Multimodal | This paper introduces a gradient-free black-box optimization method (ZO-SPSA) to effectively craft adversarial jailbreak attacks on large vision-language models, demonstrating their vulnerability to harmful outputs even without model knowledge. |
| 2026-01-04 | [Automated Post-Incident Policy Gap Analysis via Threat-Informed Evidence Mapping using Large Language Models](http://arxiv.org/abs/2601.03287v1) | LLM, Security | This paper presents a novel LLM-based framework that integrates threat-informed evidence mapping and policy gap analysis for automated cybersecurity post-incident reviews, enhancing efficiency and auditability. |
| 2026-01-01 | [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](http://arxiv.org/abs/2601.00454v1) | LLM | Defensive M2S trains guardrail models on compressed multi-turn conversations, significantly reducing computational costs for safety screening while maintaining high attack detection recall. |
| 2025-12-30 | [Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?](http://arxiv.org/abs/2512.24044v1) | LLM | This paper presents the first systematic evaluation of jailbreak attacks across the full LLM deployment pipeline, revealing that safety filters are largely effective in detection but require further refinement to optimize protection and user experience. |
| 2025-12-29 | [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](http://arxiv.org/abs/2512.23684v1) | LLM | This paper reveals that LLM-based academic reviewing systems are highly susceptible to document-level hidden prompt injection attacks, leading to substantial changes in review outcomes, with vulnerability varying significantly across different languages. |
| 2025-12-29 | [Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks](http://arxiv.org/abs/2512.23557v1) | LLM, Multimodal, Agents | This paper proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework, utilizing sanitization and validation agents coordinated by a provenance ledger, to effectively prevent multimodal prompt injection attacks in complex agentic AI systems. |
| 2025-12-29 | [EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](http://arxiv.org/abs/2512.23173v1) | LLM | This paper introduces EquaCode, a novel multi-strategy jailbreak approach that leverages equation solving and code completion to successfully bypass safety constraints in large language models, demonstrating significant vulnerabilities. |
| 2025-12-29 | [Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems](http://arxiv.org/abs/2512.23132v1) | LLM, Multimodal, Healthcare, Security | This paper systematically characterizes ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages through a multi-agent RAG system, revealing unreported threats and emphasizing the need for adaptive, ML-specific security frameworks. |
| 2025-12-29 | [It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents](http://arxiv.org/abs/2512.23128v1) | Agents | This paper introduces TRAP, a benchmark for evaluating prompt injection attacks against web-based agents, demonstrating their susceptibility and revealing systemic vulnerabilities related to persuasion techniques. |

### Robustness & Generalization
_This week's papers introduce novel methods for improving LLM robustness against adversarial attacks. One paper presents RAILS, a gradient-free and prior-free optimization framework that leverages an auto-regressive loss and history-based selection for effective jailbreaking, even against closed-source models. The other paper, JPU, focuses on a defense mechanism that rectifies dynamic jailbreak paths by mining on-policy adversarial samples, aiming to enhance LLM resistance without compromising utility._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-07 | [Neuro-Symbolic Compliance: Integrating LLMs and SMT Solvers for Automated Financial Legal Analysis](http://arxiv.org/abs/2601.06181v1) | LLM | This paper introduces a neuro-symbolic framework that leverages LLMs and SMT solvers to achieve verifiable and optimized financial legal compliance. |
| 2026-01-07 | [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](http://arxiv.org/abs/2601.03948v2) | LLM, Reinforcement Learning | Trade-R1 bridges verifiable rewards to stochastic financial environments by transforming reasoning evaluation into a RAG task with a triangular consistency metric to filter noisy rewards and reduce reward hacking. |
| 2026-01-06 | [Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks](http://arxiv.org/abs/2601.03420v1) | LLM | RAILS is a novel gradient-free and prior-free iterative optimization framework for jailbreaking LLMs that uses an auto-regressive loss and history-based selection to achieve high transferability to closed-source models. |
| 2026-01-06 | [JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification](http://arxiv.org/abs/2601.03005v1) | LLM | JPU rectifies dynamic jailbreak paths by dynamically mining on-policy adversarial samples, enhancing LLM resistance to jailbreaks without sacrificing utility. |
