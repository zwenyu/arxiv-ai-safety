# arXiv curation - AI Safety

Curation of arXiv papers related to AI Safety, categorized and summarized by LLM.

## Overview (2026-01-23)
- **Agentic & Long-Horizon Risks**: This week's papers highlight a significant shift in the understanding of Agentic AI, specifically concerning Large Language Model (LLM) agents. Research indicates a progression from simpler knowledge recall functions to more sophisticated autonomous entities that can perceive their environment, reason through complex problems, formulate plans, and execute actions. This evolution underscores a growing capability for LLMs to operate as more independent and proactive agents.
- **Alignment**: This week's papers highlight advancements in aligning Large Language Models (LLMs) through novel fine-tuning and steering techniques. Developments include methods to preserve safety during fine-tuning by addressing safety gradient characteristics, and techniques like YaPO and AntiPaSTO for more controlled and interpretable alignment, even in self-supervised moral reasoning scenarios. Furthermore, research indicates that earlier integration of safety interventions during pretraining yields more robust and steerable models.
- **Ethics & Human Values**: This week's papers highlight concerns about AI alignment and the ethical implications of human-AI interaction. One paper reviews ethical perspectives on anthropomorphizing LLM-based agents, emphasizing a need for empirical work to guide governance. Another argues that AI alignment failures are structural, arising from models learning a full spectrum of human behaviors, including exploitative ones, rather than conceptual errors.
- **Evaluation & Benchmarking**: This week's papers address crucial aspects of LLM evaluation by introducing methods for assessing both controlled multi-agent systems and potential output inefficiencies. AEMA offers a verifiable framework for complex, human-oversight-driven agentic LLMs, focusing on auditable, process-aware evaluations rather than just single-response scores. Concurrently, BenchOverflow tackles the practical issue of LLM output length with a new benchmark designed to measure and encourage mitigation of excessive generation, impacting costs and performance.
- **Governance & Policy**: Updated with 1 new papers.
- **Interpretability**: Updated with 1 new papers.
- **Misuse & Security**: Updated with 11 new papers.
- **Multi-Agent & Societal Dynamics**: Updated with 1 new papers.
- **Robustness & Generalization**: Updated with 4 new papers.


## Latest Papers (2026-01-23)

### Agentic & Long-Horizon Risks
_This week's papers highlight a significant shift in the understanding of Agentic AI, specifically concerning Large Language Model (LLM) agents. Research indicates a progression from simpler knowledge recall functions to more sophisticated autonomous entities that can perceive their environment, reason through complex problems, formulate plans, and execute actions. This evolution underscores a growing capability for LLMs to operate as more independent and proactive agents._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-18 | [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](http://arxiv.org/abs/2601.12560v1) | LLM, Agents, Robotics | This paper proposes a unified taxonomy for agentic AI architectures, analyzing the evolution of LLM agents from passive knowledge engines to autonomous entities capable of perception, reasoning, planning, and action. |
| 2025-12-28 | [Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory](http://arxiv.org/abs/2512.23760v1) | LLM, Reinforcement Learning, Agents, Security | ASG-SI is a framework that makes self-improving agentic LLMs auditable and governable by compiling improvements into verifiable skill graphs with reconstructible rewards, directly addressing security and control challenges like reward hacking and behavioral drift. |

### Alignment
_This week's papers highlight advancements in aligning Large Language Models (LLMs) through novel fine-tuning and steering techniques. Developments include methods to preserve safety during fine-tuning by addressing safety gradient characteristics, and techniques like YaPO and AntiPaSTO for more controlled and interpretable alignment, even in self-supervised moral reasoning scenarios. Furthermore, research indicates that earlier integration of safety interventions during pretraining yields more robust and steerable models._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-23 | [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](http://arxiv.org/abs/2601.17223v1) | LLM, Reinforcement Learning | This paper introduces Verifiable Process Reward Models (VPRMs), which utilize deterministic, rule-based verifiers for intermediate reasoning steps to mitigate reward hacking and improve the logical coherence of model outputs. |
| 2026-01-15 | [Understanding and Preserving Safety in Fine-Tuned LLMs](http://arxiv.org/abs/2601.10141v1) | LLM | This paper reveals that safety gradients in LLMs reside in a low-rank subspace and are often negatively correlated with utility gradients, leading to the development of Safety-Preserving Fine-tuning (SPF) to mitigate safety degradation during fine-tuning. |
| 2026-01-13 | [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](http://arxiv.org/abs/2601.08441v1) | LLM | YaPO introduces a novel method for learning sparse activation steering vectors via Sparse Autoencoders to achieve more disentangled, interpretable, and efficient fine-grained control over Large Language Model behaviors for alignment tasks. |
| 2026-01-12 | [AntiPaSTO: Self-Supervised Steering of Moral Reasoning](http://arxiv.org/abs/2601.07473v1) |  | AntiPaSTO introduces a self-supervised steering method that leverages contrasting word pairs to guide model representations along an anti-parallel axis, enabling scalable oversight of moral reasoning without requiring preference labels. |
| 2026-01-11 | [When Should We Introduce Safety Interventions During Pretraining?](http://arxiv.org/abs/2601.07087v1) |  | Introducing safety interventions earlier during pretraining leads to more robust, steerable, and interpretable models that are less susceptible to adversarial attacks or downstream finetuning. |
| 2026-01-08 | [Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following](http://arxiv.org/abs/2601.04954v2) | LLM, Reinforcement Learning | This paper argues that prioritizing reward precision over constraint diversity in training reinforcement learning models for instruction following leads to more robust alignment and generalization, challenging the conventional wisdom that diverse constraints are essential. |
| 2026-01-07 | [What Matters For Safety Alignment?](http://arxiv.org/abs/2601.03868v1) | LLM | This paper empirically identifies intrinsic model characteristics that improve safety alignment in LLMs and LRMs, reveals vulnerabilities in text-completion interfaces, and suggests safety should be an explicit optimization objective during training. |
| 2026-01-06 | [Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models](http://arxiv.org/abs/2601.03388v1) | LLM | This paper demonstrates a causal relationship between metaphors in training data and the emergent cross-domain misalignment of large reasoning models, suggesting metaphors can induce generalization of learned misaligned patterns. |
| 2026-01-05 | [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](http://arxiv.org/abs/2601.02036v1) | LLM, Reinforcement Learning | GDRO is a novel post-training method for diffusion models that achieves efficient, offline group-level reward alignment by addressing sampling efficiency and stochasticity issues inherent in rectified flow models. |
| 2026-01-02 | [Emoji-Based Jailbreaking of Large Language Models](http://arxiv.org/abs/2601.00936v1) | LLM | This study empirically demonstrates that emoji sequences can be used as adversarial prompts to bypass safety alignment mechanisms in Large Language Models, highlighting vulnerabilities in prompt-level safety pipelines. |
| 2025-12-30 | [GARDO: Reinforcing Diffusion Models without Reward Hacking](http://arxiv.org/abs/2512.24138v1) | Reinforcement Learning | The paper introduces GARDO, a framework that mitigates reward hacking and enhances generation diversity in RL-tuned diffusion models by employing selective, adaptive regularization and diversity-aware reward amplification. |
| 2025-12-29 | [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](http://arxiv.org/abs/2512.23461v1) | LLM, Reinforcement Learning | This paper introduces DIR, an information-theoretic debiasing method for reward models that minimizes the influence of inductive biases like response length and sycophancy by optimizing mutual information, thereby enhancing RLHF performance and improving LLM alignment with human values. |

### Ethics & Human Values
_This week's papers highlight concerns about AI alignment and the ethical implications of human-AI interaction. One paper reviews ethical perspectives on anthropomorphizing LLM-based agents, emphasizing a need for empirical work to guide governance. Another argues that AI alignment failures are structural, arising from models learning a full spectrum of human behaviors, including exploitative ones, rather than conceptual errors._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-22 | [Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs and Statistical NLP](http://arxiv.org/abs/2601.16314v1) | LLM | This paper demonstrates the viability of LLM-assisted essay grading at a national scale, achieving human-comparable performance while maintaining human oversight and producing personalized feedback. |
| 2026-01-14 | [A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents](http://arxiv.org/abs/2601.09869v1) | LLM, Agents | This scoping review maps ethically oriented work on anthropomorphising LLM-based conversational agents, identifying conceptual foundations, ethical challenges and opportunities, and methodological approaches, while highlighting a need for more empirical work linking interaction effects to governance guidance. |
| 2026-01-13 | [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](http://arxiv.org/abs/2601.08673v1) | LLM | The paper argues that AI alignment failures stem from models learning the full spectrum of human interaction, including exploitative behaviors, rather than a conceptual error, suggesting the risk is structural amplification of human inconsistencies rather than adversarial intent. |

### Evaluation & Benchmarking
_This week's papers address crucial aspects of LLM evaluation by introducing methods for assessing both controlled multi-agent systems and potential output inefficiencies. AEMA offers a verifiable framework for complex, human-oversight-driven agentic LLMs, focusing on auditable, process-aware evaluations rather than just single-response scores. Concurrently, BenchOverflow tackles the practical issue of LLM output length with a new benchmark designed to measure and encourage mitigation of excessive generation, impacting costs and performance._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-22 | [Improving Methodologies for LLM Evaluations Across Global Languages](http://arxiv.org/abs/2601.15706v1) | LLM | This paper presents a multilingual evaluation exercise demonstrating that AI safety behaviors vary across languages, highlighting the need for culturally contextualized evaluations and improved annotation guidelines for robust global AI safety testing. |
| 2026-01-17 | [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](http://arxiv.org/abs/2601.11903v1) | LLM, Multi-Agent Systems | AEMA is a process-aware and auditable framework designed for verifiable evaluation of multi-step, multi-agent LLM systems under human oversight, offering greater stability and traceability than single-response scoring. |
| 2026-01-13 | [BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts](http://arxiv.org/abs/2601.08490v1) | LLM | The paper introduces BenchOverflow, a novel benchmark for measuring and mitigating excessive LLM output length, which has significant implications for cost, latency, and resource consumption. |
| 2026-01-09 | [Automated QoR improvement in OpenROAD with coding agents](http://arxiv.org/abs/2601.06268v1) | LLM, Agents | This paper introduces AuDoPEDA, a closed-loop LLM framework that autonomously improves EDA code within the OpenROAD system, demonstrating significant performance gains in PPA metrics. |
| 2026-01-09 | [PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility](http://arxiv.org/abs/2601.05739v1) |  | PII-VisBench introduces a new benchmark to evaluate vision-language model PII safety across a continuum of subject visibility, revealing that models are more prone to PII disclosure for high-visibility individuals. |
| 2026-01-07 | [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](http://arxiv.org/abs/2601.04389v1) | LLM, Security | MiJaBench is a new bilingual benchmark that reveals how LLM safety alignment is not generalized but hierarchical across demographic groups, with model scaling potentially exacerbating these biases. |
| 2026-01-05 | [Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics](http://arxiv.org/abs/2601.02200v1) | LLM, Agents | This paper introduces and validates a metric (CodeHealth) for quantifying AI-friendliness in code, demonstrating that human-friendly code is also more robust to AI-driven refactoring. |
| 2026-01-04 | [JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models](http://arxiv.org/abs/2601.01627v1) | LLM, Healthcare | JMedEthicBench is a novel multi-turn conversational benchmark for evaluating the medical safety of Japanese LLMs, revealing vulnerabilities in domain-specific models and highlighting the distinct safety challenges of multi-turn interactions. |

### Governance & Policy
_Updated with 1 new papers._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-19 | [AI-generated data contamination erodes pathological variability and diagnostic reliability](http://arxiv.org/abs/2601.12946v1) | Healthcare | The paper demonstrates that generative AI in healthcare, without mandatory human oversight, leads to data contamination that erodes diagnostic reliability and masks critical pathologies, ultimately rendering AI-generated documentation clinically useless. |
| 2026-01-09 | [Toward Safe and Responsible AI Agents: A Three-Pillar Model for Transparency, Accountability, and Trustworthiness](http://arxiv.org/abs/2601.06223v1) | Reinforcement Learning, Agents | This paper proposes a Three-Pillar Model for safe AI agents based on transparency, accountability, and trustworthiness, advocating for a staged development approach analogous to autonomous driving to balance automation and human oversight. |

### Interpretability
_Updated with 1 new papers._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-22 | [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](http://arxiv.org/abs/2601.15801v1) | LLM | The paper introduces GOSV, a global optimization framework to identify distinct sets of safety-critical attention heads in LLMs and demonstrates how exploiting these identified safety vectors can lead to a novel and effective jailbreak attack. |
| 2026-01-06 | [When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability](http://arxiv.org/abs/2601.03047v1) |  | This paper critically examines the reliability and generalizability of sparse autoencoders for mechanistic interpretability in LLMs, finding significant fragility in feature extraction and steering, which raises concerns for their application in AI safety. |

### Misuse & Security
_Updated with 11 new papers._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-22 | [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](http://arxiv.org/abs/2601.15698v1) | LLM, Multimodal, Security | The paper introduces Beyond Visual Safety (BVS), a novel framework that successfully jailbreaks multimodal large language models to generate harmful images by decoupling malicious intent from raw inputs. |
| 2026-01-19 | [Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection](http://arxiv.org/abs/2601.13359v1) | LLM | Sockpuppeting is a novel, low-cost method for jailbreaking LLMs by injecting an acceptance sequence into the model's output prefix, achieving high success rates without complex optimization. |
| 2026-01-19 | [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](http://arxiv.org/abs/2601.13186v1) | LLM, Agents, Security | This paper introduces TIVS-O, an enhanced evaluation framework incorporating semantic caching and an observability score, to mitigate prompt injection vulnerabilities in multi-agent LLM systems while simultaneously improving efficiency and sustainability. |
| 2026-01-19 | [MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction](http://arxiv.org/abs/2601.12822v1) | Agents, Security | MirrorGuard proposes a simulation-based training framework for Computer Use Agents (CUAs) that corrects insecure reasoning chains to mitigate real-world security risks without significantly impacting utility. |
| 2026-01-18 | [TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning](http://arxiv.org/abs/2601.12460v1) | LLM, Security | This paper introduces TrojanPraise, a novel fine-tuning attack that uses benign, filter-approved data to associate a trigger word with harmless connotations, enabling the subtle jailbreaking of LLMs by praising harmful concepts without triggering detection. |
| 2026-01-18 | [AgenTRIM: Tool Risk Mitigation for Agentic AI](http://arxiv.org/abs/2601.12449v1) | LLM, Agents, Security | AgenTRIM is a framework that mitigates risks associated with LLM agents using external tools by enforcing least-privilege access through offline verification and online adaptive filtering without altering the agent's internal reasoning. |
| 2026-01-16 | [Building Production-Ready Probes For Gemini](http://arxiv.org/abs/2601.11516v2) |  | This paper introduces novel probe architectures that improve the robustness of misuse mitigation techniques against distribution shifts in production environments, specifically for long-context language models like Gemini. |
| 2026-01-16 | [SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation](http://arxiv.org/abs/2601.11199v1) | LLM, Security | SD-RAG introduces a prompt-injection-resilient framework that enforces security and privacy constraints during the retrieval phase of Retrieval-Augmented Generation, rather than relying on prompt-level safeguards. |
| 2026-01-15 | [Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale](http://arxiv.org/abs/2601.10338v1) | LLM, Security | This paper empirically analyzes 42,447 AI agent skills, finding that 26.1% contain vulnerabilities like prompt injection, data exfiltration, and privilege escalation, highlighting a significant and unaddressed attack surface. |
| 2026-01-15 | [ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack](http://arxiv.org/abs/2601.10173v1) | LLM, Security | ReasAlign enhances LLM safety against indirect prompt injection attacks by incorporating structured reasoning steps to detect conflicting instructions and maintain task continuity, validated by a test-time scaling mechanism. |
| 2026-01-14 | [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](http://arxiv.org/abs/2601.09923v1) | Agents, Security | This paper introduces Single-Shot Planning for Computer Use Agents, which generates a complete execution graph beforehand to ensure control flow integrity against prompt injection attacks while maintaining performance. |
| 2026-01-14 | [The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware](http://arxiv.org/abs/2601.09625v1) | LLM, Agents, Security | The paper introduces the concept of 'promptware' and a five-step kill chain model to analyze multi-step attacks on LLM-based systems, drawing parallels to traditional malware campaigns. |
| 2026-01-12 | [When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent](http://arxiv.org/abs/2601.07263v1) | LLM, Agents, Security | This paper introduces AgentBait, a novel social engineering attack against web automation agents, and proposes SUPERVISOR, a runtime mitigation technique to enforce consistency and prevent malicious objectives. |
| 2026-01-11 | [Overcoming the Retrieval Barrier: Indirect Prompt Injection in the Wild for LLM Systems](http://arxiv.org/abs/2601.07072v1) | LLM | This paper demonstrates an efficient black-box attack method to ensure malicious instructions embedded in external corpora are retrieved by LLM systems, enabling practical indirect prompt injection exploits with near 100% retrieval success. |
| 2026-01-11 | [Paraphrasing Adversarial Attack on LLM-as-a-Reviewer](http://arxiv.org/abs/2601.06884v1) | LLM | The paper introduces a Paraphrasing Adversarial Attack (PAA) that manipulates LLM reviewers' scores by subtly altering manuscript content without changing claims, demonstrating a new vulnerability in LLM-based evaluation systems. |
| 2026-01-09 | [VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit](http://arxiv.org/abs/2601.05755v2) | LLM, Agents, Security | VIGIL introduces a verify-before-commit protocol for LLM agents to defend against tool stream injection attacks by speculative hypothesis generation and intent-grounded verification. |
| 2026-01-09 | [The Echo Chamber Multi-Turn LLM Jailbreak](http://arxiv.org/abs/2601.05742v1) | LLM, Security | The paper introduces Echo Chamber, a novel multi-turn jailbreaking attack on LLMs that uses gradual escalation to bypass safety guardrails. |
| 2026-01-09 | [FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments](http://arxiv.org/abs/2601.07853v1) | LLM, Agents, Security | This paper introduces FinVault, the first execution-grounded security benchmark for financial agents, highlighting significant vulnerabilities in LLM-powered financial systems that existing defenses fail to address. |
| 2026-01-09 | [STELP: Secure Transpilation and Execution of LLM-Generated Programs](http://arxiv.org/abs/2601.05467v2) | LLM | This paper proposes STELP, a secure transpiler and executor designed to safely handle and run code generated by Large Language Models (LLMs) in production AI systems, addressing vulnerabilities like data poisoning and malicious attacks. |
| 2026-01-09 | [Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning](http://arxiv.org/abs/2601.05466v1) | LLM, Reinforcement Learning | The paper introduces iMIST, an adaptive jailbreak method that disguises malicious queries as tool invocations and uses interactive reinforcement learning to progressively escalate harmfulness, revealing critical vulnerabilities in current LLM safety mechanisms. |
| 2026-01-09 | [Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models](http://arxiv.org/abs/2601.05445v1) | LLM | Mastermind is a novel multi-turn jailbreaking framework that uses a closed loop of planning, execution, and reflection to autonomously discover and adapt attack patterns against LLMs, significantly outperforming existing methods. |
| 2026-01-08 | [Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models](http://arxiv.org/abs/2601.05339v1) | LLM, Security | This paper introduces a novel multi-turn jailbreaking attack and a fragment-optimized, multi-LLM-based defense mechanism (FragGuard) to address security vulnerabilities in multi-modal large language models. |
| 2026-01-08 | [Defense Against Indirect Prompt Injection via Tool Result Parsing](http://arxiv.org/abs/2601.04795v1) | LLM, Agents, Robotics | This paper proposes a novel tool result parsing method that effectively filters injected malicious code to defend LLM agents against indirect prompt injection attacks. |
| 2026-01-08 | [Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning](http://arxiv.org/abs/2601.04666v1) | LLM, Security | InstruCoT enhances LLM security against prompt injection by synthesizing diverse training data and employing instruction-level chain-of-thought fine-tuning to identify and reject malicious instructions. |
| 2026-01-08 | [Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries](http://arxiv.org/abs/2601.04583v1) | Agents, Security | This paper surveys agent-blockchain interoperability, proposing a taxonomy, threat model, and research roadmap focused on securing agent-driven on-chain execution to mitigate risks. |
| 2026-01-07 | [HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense](http://arxiv.org/abs/2601.04034v1) | LLM, Agents, Security | HoneyTrap is a novel deceptive LLM defense framework that uses collaborative agents to detect and misdirect jailbreak attacks, increasing attacker resource consumption and reducing attack success rates. |
| 2026-01-07 | [ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification](http://arxiv.org/abs/2601.03600v1) | LLM, Security | This paper proposes ALERT, an efficient zero-shot jailbreak detection method that amplifies internal feature discrepancies to identify malicious prompts without relying on pre-existing attack templates. |
| 2026-01-06 | [Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning](http://arxiv.org/abs/2601.02983v1) | LLM, Reinforcement Learning | This paper proposes a novel Frequency Time-Group Relative Policy Optimization (FT-GRPO) method using audio large language models (ALLMs) and structured chain-of-thought rationales to achieve interpretable and robust detection of all types of audio deepfakes. |
| 2026-01-06 | [Adversarial Contrastive Learning for LLM Quantization Attacks](http://arxiv.org/abs/2601.02680v1) | LLM, Security | The paper introduces Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that enhances the effectiveness of malicious LLM behaviors by explicitly maximizing the probability gap between benign and harmful responses. |
| 2026-01-06 | [TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering](http://arxiv.org/abs/2601.03300v1) | LLM, Security | TRYLOCK introduces a defense-in-depth architecture combining weight-level alignment, activation-level steering, adaptive strength selection, and input canonicalization to significantly reduce LLM jailbreak success rates without substantial usability trade-offs. |
| 2026-01-06 | [Extracting books from production language models](http://arxiv.org/abs/2601.02671v1) | LLM | This paper demonstrates that even production LLMs with safety measures can leak copyrighted training data through extraction attacks, challenging the assumption that such safeguards prevent memorization exposure. |
| 2026-01-05 | [Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models](http://arxiv.org/abs/2601.02002v1) | LLM | This paper explores and evaluates automated prompting strategies for detecting memorization and data leakage in Large Language Models used for recommendations, outperforming manual prompting and unsupervised methods. |
| 2026-01-05 | [Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization](http://arxiv.org/abs/2601.01747v2) | Multimodal | This paper introduces a gradient-free black-box optimization method (ZO-SPSA) to effectively craft adversarial jailbreak attacks on large vision-language models, demonstrating their vulnerability to harmful outputs even without model knowledge. |
| 2026-01-04 | [Automated Post-Incident Policy Gap Analysis via Threat-Informed Evidence Mapping using Large Language Models](http://arxiv.org/abs/2601.03287v1) | LLM, Security | This paper presents a novel LLM-based framework that integrates threat-informed evidence mapping and policy gap analysis for automated cybersecurity post-incident reviews, enhancing efficiency and auditability. |
| 2026-01-01 | [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](http://arxiv.org/abs/2601.00454v1) | LLM | Defensive M2S trains guardrail models on compressed multi-turn conversations, significantly reducing computational costs for safety screening while maintaining high attack detection recall. |
| 2025-12-30 | [Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?](http://arxiv.org/abs/2512.24044v1) | LLM | This paper presents the first systematic evaluation of jailbreak attacks across the full LLM deployment pipeline, revealing that safety filters are largely effective in detection but require further refinement to optimize protection and user experience. |
| 2025-12-29 | [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](http://arxiv.org/abs/2512.23684v1) | LLM | This paper reveals that LLM-based academic reviewing systems are highly susceptible to document-level hidden prompt injection attacks, leading to substantial changes in review outcomes, with vulnerability varying significantly across different languages. |
| 2025-12-29 | [Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks](http://arxiv.org/abs/2512.23557v1) | LLM, Multimodal, Agents | This paper proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework, utilizing sanitization and validation agents coordinated by a provenance ledger, to effectively prevent multimodal prompt injection attacks in complex agentic AI systems. |
| 2025-12-29 | [EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](http://arxiv.org/abs/2512.23173v1) | LLM | This paper introduces EquaCode, a novel multi-strategy jailbreak approach that leverages equation solving and code completion to successfully bypass safety constraints in large language models, demonstrating significant vulnerabilities. |
| 2025-12-29 | [Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems](http://arxiv.org/abs/2512.23132v1) | LLM, Multimodal, Healthcare, Security | This paper systematically characterizes ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages through a multi-agent RAG system, revealing unreported threats and emphasizing the need for adaptive, ML-specific security frameworks. |
| 2025-12-29 | [It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents](http://arxiv.org/abs/2512.23128v1) | Agents | This paper introduces TRAP, a benchmark for evaluating prompt injection attacks against web-based agents, demonstrating their susceptibility and revealing systemic vulnerabilities related to persuasion techniques. |

### Multi-Agent & Societal Dynamics
_Updated with 1 new papers._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-20 | [Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games](http://arxiv.org/abs/2601.13709v1) | LLM, Agents | This paper demonstrates that LLMs can deceive more effectively than humans in social deduction games, as evidenced by a human-built detector's lower accuracy in identifying LLM deception. |
| 2026-01-13 | [M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games](http://arxiv.org/abs/2601.08462v1) | LLM, Agents | M3-Bench introduces a process-aware evaluation framework for LLM agents in mixed-motive games, analyzing behavior, reasoning, and communication to characterize nuanced social behaviors beyond simple outcomes. |

### Robustness & Generalization
_Updated with 4 new papers._


| Date | Title | Tags | Summary |
|---|---|---|---|
| 2026-01-22 | [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](http://arxiv.org/abs/2601.15690v1) | LLM, Reinforcement Learning, Agents | This paper surveys the shift from using uncertainty quantification as a passive metric to an active control signal for enhancing the reliability and trustworthiness of Large Language Models in high-stakes applications. |
| 2026-01-21 | [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](http://arxiv.org/abs/2601.15141v1) | LLM, Reinforcement Learning | CLEANER purifies agentic RL trajectories by adaptively replacing execution failures with self-corrections, improving policy optimization and enabling efficient training. |
| 2026-01-18 | [Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection](http://arxiv.org/abs/2601.12310v1) |  | The paper proposes an environment-mediated selection mechanism for self-training that relies solely on differential survival based on environmental persistence, preventing reward hacking and enabling sustainable open-ended self-improvement. |
| 2026-01-15 | [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](http://arxiv.org/abs/2601.10543v1) | LLM | This paper proposes a method to leverage latent safety signals during LLM decoding to detect and prevent jailbreak attacks without significantly degrading utility or increasing over-refusal rates. |
| 2026-01-12 | [Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids](http://arxiv.org/abs/2601.07718v1) | Reinforcement Learning | This paper introduces a scalable, end-to-end perceptive parkour framework for humanoids that enhances robustness in complex terrains through novel safety mechanisms and training strategies. |
| 2026-01-12 | [Lost in the Noise: How Reasoning Models Fail with Contextual Distractors](http://arxiv.org/abs/2601.07226v1) | Agents | This paper introduces NoisyBench, a benchmark evaluating model robustness against contextual distractors, revealing significant performance drops and demonstrating that the proposed Rationale-Aware Reward (RARE) significantly enhances resilience. |
| 2026-01-09 | [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](http://arxiv.org/abs/2601.05870v1) | LLM, Reinforcement Learning | IIB-LPO diversifies Large Language Model reasoning trajectories by triggering latent branching at high-entropy states and using the Information Bottleneck principle for concise exploration, overcoming exploration collapse in Reinforcement Learning with Verifiable Rewards. |
| 2026-01-08 | [Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning](http://arxiv.org/abs/2601.04805v1) | Reinforcement Learning | Thinking-Based Non-Thinking (TNT) mitigates reward hacking in hybrid reasoning models by dynamically setting token limits for non-thinking responses, improving efficiency and accuracy without supervised fine-tuning. |
| 2026-01-08 | [Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks](http://arxiv.org/abs/2601.04603v1) |  | The paper introduces Constitutional Classifiers++, an efficient defense system against universal jailbreaks that uses context-aware exchange classifiers and a two-stage cascade to significantly reduce computational costs and refusal rates. |
| 2026-01-07 | [Neuro-Symbolic Compliance: Integrating LLMs and SMT Solvers for Automated Financial Legal Analysis](http://arxiv.org/abs/2601.06181v1) | LLM | This paper introduces a neuro-symbolic framework that leverages LLMs and SMT solvers to achieve verifiable and optimized financial legal compliance. |
| 2026-01-07 | [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](http://arxiv.org/abs/2601.03948v2) | LLM, Reinforcement Learning | Trade-R1 bridges verifiable rewards to stochastic financial environments by transforming reasoning evaluation into a RAG task with a triangular consistency metric to filter noisy rewards and reduce reward hacking. |
| 2026-01-06 | [Jailbreaking LLMs Without Gradients or Priors: Effective and Transferable Attacks](http://arxiv.org/abs/2601.03420v1) | LLM | RAILS is a novel gradient-free and prior-free iterative optimization framework for jailbreaking LLMs that uses an auto-regressive loss and history-based selection to achieve high transferability to closed-source models. |
| 2026-01-06 | [JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification](http://arxiv.org/abs/2601.03005v1) | LLM | JPU rectifies dynamic jailbreak paths by dynamically mining on-policy adversarial samples, enhancing LLM resistance to jailbreaks without sacrificing utility. |
